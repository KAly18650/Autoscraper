# AutoScraper: AI-Powered Multi-Agent Web Scraping Pipeline

> **Automatically generate production-ready web scrapers using coordinated AI agents**

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/)
[![Google ADK](https://img.shields.io/badge/Google-ADK-4285F4.svg)](https://ai.google.dev/adk)
[![Gemini AI](https://img.shields.io/badge/Gemini-AI-8E75B2.svg)](https://ai.google.dev/)

AutoScraper transforms web scraping from a manual, time-consuming process into an autonomous, AI-driven workflow. Using a team of specialized agents, it analyzes websites, generates Python code, validates output, and saves reusable scrapersâ€”all in minutes.

---

## ğŸ¯ Problem & Solution

**The Problem:** Creating web scrapers traditionally requires:
- Hours of HTML inspection to find the right selectors
- Writing complex Python code with error handling
- Extensive testing and debugging
- Maintenance when websites change

**The Solution:** AutoScraper automates this entire workflow using coordinated AI agents that work like a real development teamâ€”analyst, coder, and QA engineerâ€”orchestrated by an intelligent project manager.

---

## ğŸ—ï¸ Architecture

AutoScraper uses a **hierarchical multi-agent system** with four specialized agents:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Orchestrator Agent                    â”‚
â”‚         (Project Manager)                       â”‚
â”‚  â€¢ Coordinates workflow                         â”‚
â”‚  â€¢ Routes tasks to specialists                  â”‚
â”‚  â€¢ Handles error classification                 â”‚
â”‚  â€¢ Manages iterative refinement                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚              â”‚              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Analyst   â”‚ â”‚   Coder    â”‚ â”‚  Validator  â”‚
    â”‚  Agent     â”‚ â”‚   Agent    â”‚ â”‚   Agent     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ â€¢ Analyzes â”‚ â”‚ â€¢ Generatesâ”‚ â”‚ â€¢ Executes  â”‚
    â”‚   HTML     â”‚ â”‚   Python   â”‚ â”‚   code      â”‚
    â”‚ â€¢ Tests    â”‚ â”‚   code     â”‚ â”‚ â€¢ Validates â”‚
    â”‚   selectorsâ”‚ â”‚ â€¢ Error    â”‚ â”‚   output    â”‚
    â”‚ â€¢ Returns  â”‚ â”‚   handling â”‚ â”‚ â€¢ Classifiesâ”‚
    â”‚   Selector â”‚ â”‚ â€¢ Best     â”‚ â”‚   errors    â”‚
    â”‚   Map      â”‚ â”‚   practicesâ”‚ â”‚             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚              â”‚              â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                  â”‚  Scraper    â”‚
                  â”‚  Repository â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Features

âœ… **Two Scraper Types**
- **Content Scrapers**: Extract data from individual pages (title, author, content, etc.)
- **List Scrapers**: Extract URLs from listing pages, then scrape each URL

âœ… **Intelligent Error Routing**
- Coding errors â†’ Route back to Coder Agent
- Selector errors â†’ Route back to Analyst Agent
- Max 5 iterations with automatic refinement

âœ… **Reusable Repository**
- Validated scrapers saved with metadata
- Auto-detection from URLs
- Pipeline support (list + content scrapers)

âœ… **Production-Ready**
- Docker containerization
- Google Cloud Storage integration
- Cloud Run deployment
- Comprehensive error handling

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.12+
- Google Gemini API key ([Get one here](https://ai.google.dev/))

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/autoscraper.git
   cd autoscraper
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   playwright install chromium
   ```

3. **Configure environment**
   ```bash
   cp .env.example .env
   # Edit .env and add your GEMINI_API_KEY
   ```

4. **Verify setup**
   ```bash
   python tests/test_env_integrations.py
   python tests/test_playwright.py
   ```

---

## ğŸ’» Usage

### Command Line Interface

Create a scraper for any website:

```bash
python main.py --url "https://hms.harvard.edu/news/article" \
               --prompt "Extract title, author, date, and content"
```

**What happens:**
1. Orchestrator coordinates the workflow
2. Analyst analyzes HTML and identifies selectors
3. Coder generates Python scraper code
4. Validator tests and validates the scraper
5. Scraper saved to `scraper_repository/scrapers/`

### ADK Web Interface

For interactive scraper creation:

```bash
adk web
```

Then open http://localhost:8000 and chat with the orchestrator agent.

### Using Saved Scrapers

Once generated, scrapers are reusable:

```python
from scraper_repository import get_scraper

# Get scraper by domain
scraper = get_scraper("hms.harvard.edu")
data = scraper.scrape("https://hms.harvard.edu/news/any-article")

print(data)
# {'title': '...', 'author': '...', 'publish_date': '...', 'content': '...'}
```

### Two-Step Pipeline

For news sites and blogs:

```python
from scraper_repository import get_scraper_pipeline

# Get both list and content scrapers
list_scraper, content_scraper = get_scraper_pipeline("hms.harvard.edu")

# Step 1: Get all article URLs
urls = list_scraper.scrape("https://hms.harvard.edu/news")["urls"]

# Step 2: Scrape each article
articles = [content_scraper.scrape(url) for url in urls]
```

---

## ğŸ“ Project Structure

```
autoscraper/
â”œâ”€â”€ orchestrator/          # Orchestrator agent (coordinates workflow)
â”œâ”€â”€ analyst/              # Analyst agent (HTML analysis)
â”œâ”€â”€ coder/                # Coder agent (code generation)
â”œâ”€â”€ validator/            # Validator agent (testing & validation)
â”œâ”€â”€ shared/               # Shared tools and utilities
â”‚   â”œâ”€â”€ tools.py         # Agent tools (analyze_html, test_selector, execute_code)
â”‚   â”œâ”€â”€ storage.py       # GCS + local storage abstraction
â”‚   â”œâ”€â”€ config.py        # Configuration management
â”‚   â””â”€â”€ logger.py        # Structured logging
â”œâ”€â”€ scraper_repository/   # Generated scrapers
â”‚   â”œâ”€â”€ scrapers/        # Python scraper modules
â”‚   â”œâ”€â”€ metadata/        # Scraper metadata (JSON)
â”‚   â””â”€â”€ __init__.py      # Repository API
â”œâ”€â”€ examples/             # Usage examples
â”œâ”€â”€ tests/                # Diagnostic tests
â”œâ”€â”€ main.py              # CLI entry point
â”œâ”€â”€ Dockerfile           # Container definition
â””â”€â”€ deploy_cloud_run.ps1 # Deployment script
```

---

## ğŸ› ï¸ Technologies

| Category | Technologies |
|----------|-------------|
| **AI Framework** | Google ADK, Gemini 2.5 Flash, Gemini 2.0 Flash |
| **Web Scraping** | Playwright, BeautifulSoup4 |
| **Cloud** | Google Cloud Storage, Cloud Run, Secret Manager |
| **Development** | Python 3.12, Docker, PowerShell |

---

## ğŸ“š Documentation

- **[PROJECT_DESCRIPTION.md](PROJECT_DESCRIPTION.md)** - Detailed competition submission (problem statement, architecture, demo)
- **[DEPLOYMENT.md](DEPLOYMENT.md)** - Cloud deployment guide (GCP setup, Cloud Run deployment)
- **[examples/README.md](examples/README.md)** - Usage examples and patterns
- **[tests/README.md](tests/README.md)** - Diagnostic tests and troubleshooting

---

## ğŸ§ª Testing

Run diagnostic tests to verify your setup:

```bash
# Test environment configuration
python tests/test_env_integrations.py

# Test Playwright installation
python tests/test_playwright.py

# Test generated scrapers
python tests/test_scrapers.py

# Test agent tools
python tests/test_tools.py
```

---

## ğŸš¢ Deployment

Deploy to Google Cloud Run:

1. **Configure `.env`** with your GCP project details
2. **Run deployment script:**
   ```powershell
   .\deploy_cloud_run.ps1
   ```

See [DEPLOYMENT.md](DEPLOYMENT.md) for detailed instructions.

---

## ğŸ“ Examples

### Example 1: Single-Page Scraper
```bash
python main.py --url "https://example.com/article" \
               --prompt "Extract title and content"
```

### Example 2: List Scraper
```bash
python main.py --url "https://example.com/blog" \
               --prompt "Create a list scraper for blog post URLs"
```

### Example 3: Using Repository
```bash
python examples/example_use_repository.py
```

### Example 4: Pipeline Workflow
```bash
python examples/example_use_pipeline.py
```

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

---

## ğŸ“„ License

This project is licensed under the MIT License - see LICENSE file for details.

---

## ğŸ™ Acknowledgments

- Built with [Google ADK](https://ai.google.dev/adk)
- Powered by [Gemini AI](https://ai.google.dev/)
- Created for the 5-Day AI Agents Intensive

---

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

---

**Made with â¤ï¸ using AI Agents**
